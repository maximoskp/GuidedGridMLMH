{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124044ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_utils import GuidedGridMLMDataset, GuidedGridMLM_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from GridMLM_tokenizers import GuidedGridMLMTokenizer\n",
    "from models import GuidedMLMH\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from generate_utils import load_model, structured_progressive_generate, random_progressive_generate\n",
    "from copy import deepcopy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4824a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GuidedGridMLMTokenizer(fixed_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'cuda:2'\n",
    "train_dir = '/media/maindisk/data/hooktheory_hr/hooktheory_CA_train'\n",
    "subfolder = 'unf_CA'\n",
    "curriculum_type='random'\n",
    "ablation = 'all'\n",
    "model = load_model( curriculum_type, subfolder, ablation, device_name, tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9c73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GuidedGridMLMDataset(train_dir, tokenizer, 512, frontloading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bc1b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pianoroll', 'time_signature', 'features'])\n",
      "[268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 296, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 268, 268, 268, 268, 268, 268, 268, 268, 122, 122, 122, 122, 122, 122, 122, 122, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "d = train_dataset[0]\n",
    "print(d.keys())\n",
    "print(d['input_ids'])\n",
    "print(d['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bd77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the melody as input and the guide\n",
    "i_input = 0\n",
    "i_guide = 1\n",
    "input_encoded = train_dataset[i_input]\n",
    "guide_encoded = train_dataset[i_guide]\n",
    "\n",
    "harmony_guide = torch.LongTensor(guide_encoded['input_ids']).reshape(1, len(guide_encoded['input_ids']))\n",
    "# harmony_real = torch.LongTensor(input_encoded['input_ids']).reshape(1, len(input_encoded['input_ids']))\n",
    "melody_grid = torch.FloatTensor( input_encoded['pianoroll'] ).reshape( 1, input_encoded['pianoroll'].shape[0], input_encoded['pianoroll'].shape[1] )\n",
    "conditioning_vec = torch.FloatTensor( input_encoded['time_signature'] ).reshape( 1, len(input_encoded['time_signature']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de20c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "base2_generated_harmony = structured_progressive_generate(\n",
    "    model=model,\n",
    "    melody_grid=melody_grid.to(model.device),\n",
    "    conditioning_vec=conditioning_vec.to(model.device),\n",
    "    guiding_harmony=harmony_guide.to(model.device),\n",
    "    num_stages=10,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    temperature=1.0,\n",
    "    strategy='sample',\n",
    "    pad_token_id=tokenizer.pad_token_id,      # token ID for <pad>\n",
    "    nc_token_id=tokenizer.nc_token_id,       # token ID for <nc>\n",
    "    force_fill=True,         # disallow <pad>/<nc> before melody ends\n",
    "    chord_constraints = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c54510",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_attention = torch.logical_not( base2_generated_harmony[0] == tokenizer.pad_token_id ).long().tolist()\n",
    "new_input_ids = base2_generated_harmony[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40a5fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disentanglement_data(\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        save_folder,\n",
    "        device_name = 'cuda:2',\n",
    "        subfolder = 'unf_CA',\n",
    "        curriculum_type='random',\n",
    "        ablation = 'all',\n",
    "        num_melodies = 1000,\n",
    "        num_guides = 200,\n",
    "    ):\n",
    "    # load model\n",
    "    model = load_model( curriculum_type, subfolder, ablation, device_name, tokenizer )\n",
    "    # generation function\n",
    "    if curriculum_type == 'random':\n",
    "        gen_fun = random_progressive_generate\n",
    "    else:\n",
    "        gen_fun = structured_progressive_generate\n",
    "    # train dataset has to come from outside, to load it once for multiple simultaneous runs\n",
    "    # # load training data\n",
    "    # train_dataset = GuidedGridMLMDataset(train_dir, tokenizer, 512, frontloading=True)\n",
    "    # permutation of melody indices\n",
    "    input_idxs = np.random.permutation( len(train_dataset) )\n",
    "    # initialize new dataset\n",
    "    new_dataset = []\n",
    "    for i_input in tqdm(input_idxs[:num_melodies]):\n",
    "        # permutation of guide indices\n",
    "        # exclude the input idx\n",
    "        all_indexes = np.delete(np.arange(len(train_dataset)), i_input)\n",
    "        # permutation of the remaining indices\n",
    "        guide_idxs = np.random.permutation( all_indexes )\n",
    "        for i_guide in guide_idxs[:num_guides]:\n",
    "            input_encoded = train_dataset[i_input]\n",
    "            guide_encoded = train_dataset[i_guide]\n",
    "            harmony_guide = torch.LongTensor(guide_encoded['input_ids']).reshape(1, len(guide_encoded['input_ids']))\n",
    "            # harmony_real = torch.LongTensor(input_encoded['input_ids']).reshape(1, len(input_encoded['input_ids']))\n",
    "            melody_grid = torch.FloatTensor( input_encoded['pianoroll'] ).reshape( 1, input_encoded['pianoroll'].shape[0], input_encoded['pianoroll'].shape[1] )\n",
    "            conditioning_vec = torch.FloatTensor( input_encoded['time_signature'] ).reshape( 1, len(input_encoded['time_signature']) )\n",
    "            generated_harmony = gen_fun(\n",
    "                model=model,\n",
    "                melody_grid=melody_grid.to(model.device),\n",
    "                conditioning_vec=conditioning_vec.to(model.device),\n",
    "                guiding_harmony=harmony_guide.to(model.device),\n",
    "                num_stages=10,\n",
    "                mask_token_id=tokenizer.mask_token_id,\n",
    "                temperature=1.0,\n",
    "                strategy='sample',\n",
    "                pad_token_id=tokenizer.pad_token_id,      # token ID for <pad>\n",
    "                nc_token_id=tokenizer.nc_token_id,       # token ID for <nc>\n",
    "                force_fill=True,         # disallow <pad>/<nc> before melody ends\n",
    "                chord_constraints = None\n",
    "            )\n",
    "            new_attention = torch.logical_not( generated_harmony[0] == tokenizer.pad_token_id ).long().tolist()\n",
    "            new_input_ids = generated_harmony[0].tolist()\n",
    "            new_d = deepcopy( input_encoded )\n",
    "            new_d['input_ids'] = new_input_ids\n",
    "            new_d['attention_mask'] = new_attention\n",
    "            new_dataset.append( new_d )\n",
    "    save_folder += '_' + subfolder + '_' + curriculum_type + '_' + ablation + '.pickle'\n",
    "    with open(save_folder, 'wb') as f:\n",
    "        pickle.dump(new_dataset, f)\n",
    "    return new_dataset\n",
    "# end generate_disentanglement_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8d1d167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.27it/s]\n"
     ]
    }
   ],
   "source": [
    "save_folder = 'data/test_disentanglement'\n",
    "\n",
    "new_dataset = generate_disentanglement_data(\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    save_folder,\n",
    "    device_name = 'cuda:2',\n",
    "    subfolder = 'unf_CA',\n",
    "    curriculum_type='random',\n",
    "    ablation = 'all',\n",
    "    num_melodies = 10,\n",
    "    num_guides = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d57c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pianoroll', 'time_signature', 'features'])\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(new_dataset[0].keys())\n",
    "print(new_dataset[0]['attention_mask'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
